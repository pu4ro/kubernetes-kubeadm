# GPU ë…¸ë“œ ìžë™ ì„¤ì • ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Kubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ NVIDIA GPUê°€ ìžˆëŠ” ë…¸ë“œì˜ ìžë™ ê°ì§€ ë° containerd ì„¤ì • ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ðŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ìžë™ ê°ì§€ ë©”ì»¤ë‹ˆì¦˜](#ìžë™-ê°ì§€-ë©”ì»¤ë‹ˆì¦˜)
- [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
- [ì„¤ì • í™•ì¸](#ì„¤ì •-í™•ì¸)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
- [ìˆ˜ë™ ì„¤ì •](#ìˆ˜ë™-ì„¤ì •)

---

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **ìžë™ GPU ê°ì§€** ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. NVIDIA GPUê°€ ìžˆëŠ” ë…¸ë“œë¥¼ ìžë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ containerdë¥¼ NVIDIA Container Runtimeìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- âœ… **ìžë™ ê°ì§€**: `lspci` ëª…ë ¹ìœ¼ë¡œ NVIDIA GPU ìžë™ íƒì§€
- âœ… **ìžë™ ì„¤ì •**: GPUê°€ ê°ì§€ë˜ë©´ containerd config.tomlì„ NVIDIA ëŸ°íƒ€ìž„ìœ¼ë¡œ ìžë™ ì„¤ì •
- âœ… **Fact ìºì‹±**: ê°ì§€ ê²°ê³¼ë¥¼ factë¡œ ì €ìž¥í•˜ì—¬ ìž¬ì‚¬ìš©
- âœ… **ì—­í•  ë¶„ë¦¬**: install_os_packageì—ì„œ ê°ì§€, install_containerdì—ì„œ ì„¤ì • ì ìš©
- âœ… **ìˆ˜ë™ ì œì–´**: í•„ìš”ì‹œ ë³€ìˆ˜ë¡œ ìˆ˜ë™ ì œì–´ ê°€ëŠ¥

---

## ìžë™ ê°ì§€ ë©”ì»¤ë‹ˆì¦˜

### 1. GPU ê°ì§€ í”„ë¡œì„¸ìŠ¤

```yaml
# install_os_package role
1. lspci | grep -i NVIDIA ì‹¤í–‰
2. NVIDIA GPU ë°œê²¬ ì‹œ has_nvidia_gpu=true ì„¤ì •
3. factë¥¼ ìºì‹œì— ì €ìž¥ (cacheable: yes)
```

### 2. Containerd ì„¤ì • ì ìš©

```yaml
# install_containerd role
1. has_nvidia_gpu fact í™•ì¸
2. factê°€ ì—†ìœ¼ë©´ ë‹¤ì‹œ GPU ê°ì§€ ìˆ˜í–‰
3. GPU ìžˆìœ¼ë©´: containerd_nvidia.j2 í…œí”Œë¦¿ ì‚¬ìš©
4. GPU ì—†ìœ¼ë©´: containerd_config.toml.j2 í…œí”Œë¦¿ ì‚¬ìš©
```

### 3. ì„¤ì • ë³€ìˆ˜

| ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ | ìžë™ ì„¤ì • |
|------|------|--------|-----------|
| `has_nvidia_gpu` | NVIDIA GPU ì¡´ìž¬ ì—¬ë¶€ | - | âœ… ìžë™ ê°ì§€ |
| `nvidia_runtime_enabled` | NVIDIA ëŸ°íƒ€ìž„ í™œì„±í™” ì—¬ë¶€ | - | âœ… ë“œë¼ì´ë²„ ì„¤ì¹˜ í›„ |
| `install_gpu_driver` | GPU ë“œë¼ì´ë²„ ì„¤ì¹˜ ì—¬ë¶€ | false | âŒ ìˆ˜ë™ ì„¤ì • |

---

## ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš© (ìžë™ ê°ì§€)

ë³„ë„ì˜ ì„¤ì • ì—†ì´ playbookì„ ì‹¤í–‰í•˜ë©´ ìžë™ìœ¼ë¡œ GPUë¥¼ ê°ì§€í•©ë‹ˆë‹¤:

```bash
# ì „ì²´ í´ëŸ¬ìŠ¤í„° ì„¤ì¹˜
ansible-playbook -i inventory.ini site.yml

# íŠ¹ì • ë…¸ë“œë§Œ
ansible-playbook -i inventory.ini site.yml -l worker3
```

ì‹¤í–‰ ì¤‘ ì¶œë ¥ ì˜ˆì‹œ:

```
TASK [install_containerd : Display containerd configuration mode] ****
ok: [worker1] => {
    "msg": "Configuring containerd with standard runtime"
}
ok: [worker2] => {
    "msg": "Configuring containerd with NVIDIA GPU runtime"
}
```

### GPU ë“œë¼ì´ë²„ ìžë™ ì„¤ì¹˜

NVIDIA ë“œë¼ì´ë²„ê¹Œì§€ ìžë™ìœ¼ë¡œ ì„¤ì¹˜í•˜ë ¤ë©´:

```bash
ansible-playbook -i inventory.ini site.yml -e "install_gpu_driver=true"
```

**ìš”êµ¬ì‚¬í•­:**
- RedHat/CentOS 8.x ì´ìƒ
- Local repositoryì— NVIDIA ë“œë¼ì´ë²„ íŒŒì¼ (`NVIDIA-Linux-x86_64-*.run`) ë°°ì¹˜
- `driver_version` ë³€ìˆ˜ ì„¤ì • (`group_vars/all.yml`)

### íŠ¹ì • ë…¸ë“œë§Œ GPU ë“œë¼ì´ë²„ ì„¤ì¹˜

```bash
ansible-playbook -i inventory.ini site.yml \
  -l worker2,worker3 \
  -e "install_gpu_driver=true"
```

---

## ì„¤ì • í™•ì¸

### 1. GPU ê°ì§€ í™•ì¸

```bash
# ë…¸ë“œì—ì„œ GPU í™•ì¸
ansible -i inventory.ini all -m shell -a "lspci | grep -i NVIDIA" -b

# ì¶œë ¥ ì˜ˆì‹œ:
# worker1 | FAILED  # GPU ì—†ìŒ
# worker2 | SUCCESS | rc=0 >>
# 01:00.0 VGA compatible controller: NVIDIA Corporation ...
```

### 2. Containerd ì„¤ì • í™•ì¸

```bash
# config.toml í™•ì¸
ansible -i inventory.ini all -m shell -a "grep -A 5 nvidia /etc/containerd/config.toml" -b

# NVIDIA runtime ì„¤ì • í™•ì¸
ansible -i inventory.ini worker2 -m shell \
  -a "grep 'default_runtime_name = \"nvidia\"' /etc/containerd/config.toml" -b
```

### 3. Fact í™•ì¸

```bash
# has_nvidia_gpu fact í™•ì¸
ansible -i inventory.ini all -m setup -a "filter=ansible_local"

# ë˜ëŠ” ad-hocìœ¼ë¡œ fact ì¡°íšŒ
ansible -i inventory.ini all -m debug -a "var=has_nvidia_gpu"
```

### 4. NVIDIA ëŸ°íƒ€ìž„ í…ŒìŠ¤íŠ¸

GPUê°€ ì„¤ì •ëœ ë…¸ë“œì—ì„œ:

```bash
# nvidia-smi ì‹¤í–‰ í™•ì¸
ssh worker2 "nvidia-smi"

# containerdì—ì„œ NVIDIA ëŸ°íƒ€ìž„ ì‚¬ìš© í™•ì¸
ssh worker2 "nerdctl run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi"
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: GPUê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ:**
```
TASK [install_containerd : Display containerd configuration mode]
ok: [worker2] => {
    "msg": "Configuring containerd with standard runtime"
}
```

**í•´ê²°:**

```bash
# 1. ìˆ˜ë™ìœ¼ë¡œ GPU í™•ì¸
ssh worker2 "lspci | grep -i NVIDIA"

# 2. lspci íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
ssh worker2 "which lspci"

# 3. pciutils ì„¤ì¹˜
ansible -i inventory.ini worker2 -m package -a "name=pciutils state=present" -b

# 4. Playbook ìž¬ì‹¤í–‰
ansible-playbook -i inventory.ini site.yml -l worker2 --tags container
```

### ë¬¸ì œ 2: Containerdê°€ NVIDIA ì„¤ì •ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

**ì¦ìƒ:**
```
Error: could not select device driver "nvidia"
```

**í•´ê²°:**

```bash
# 1. nvidia-container-toolkit ì„¤ì¹˜ í™•ì¸
ssh worker2 "rpm -qa | grep nvidia-container-toolkit"

# 2. ìˆ˜ë™ìœ¼ë¡œ toolkit ì„¤ì¹˜
ansible -i inventory.ini worker2 -m yum -a "name=nvidia-container-toolkit state=present" -b

# 3. containerd ìž¬ì‹œìž‘
ansible -i inventory.ini worker2 -m systemd -a "name=containerd state=restarted" -b
```

### ë¬¸ì œ 3: Factê°€ ìºì‹œë˜ì§€ ì•ŠìŒ

**ì¦ìƒ:**
ë§¤ë²ˆ GPU ê°ì§€ë¥¼ ë‹¤ì‹œ ìˆ˜í–‰í•¨

**í•´ê²°:**

```bash
# 1. fact cache ë””ë ‰í† ë¦¬ í™•ì¸
ansible -i inventory.ini worker2 -m file -a "path=/etc/ansible/facts.d state=directory" -b

# 2. ìˆ˜ë™ìœ¼ë¡œ fact ì„¤ì •
cat > /tmp/gpu.fact << 'EOF'
#!/bin/bash
echo '{"has_nvidia_gpu": true}'
EOF

ansible -i inventory.ini worker2 -m copy \
  -a "src=/tmp/gpu.fact dest=/etc/ansible/facts.d/gpu.fact mode=0755" -b

# 3. fact ìž¬ìˆ˜ì§‘
ansible -i inventory.ini worker2 -m setup
```

### ë¬¸ì œ 4: ë“œë¼ì´ë²„ ì„¤ì¹˜ ì‹¤íŒ¨

**ì¦ìƒ:**
```
NVIDIA driver installation failed
```

**í•´ê²°:**

```bash
# 1. Nouveau ë“œë¼ì´ë²„ ë¹„í™œì„±í™” í™•ì¸
ssh worker2 "lsmod | grep nouveau"

# 2. ì»¤ë„ ê°œë°œ íŒ¨í‚¤ì§€ í™•ì¸
ssh worker2 "rpm -qa | grep kernel-devel"

# 3. ìž¬ë¶€íŒ… í›„ ìž¬ì‹œë„
ansible -i inventory.ini worker2 -m reboot -b
ansible-playbook -i inventory.ini site.yml -l worker2 -e "install_gpu_driver=true"
```

---

## ìˆ˜ë™ ì„¤ì •

ìžë™ ê°ì§€ê°€ ìž‘ë™í•˜ì§€ ì•Šê±°ë‚˜ ìˆ˜ë™ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš°:

### ë°©ë²• 1: Host Variables

`host_vars/worker2.yml` ìƒì„±:

```yaml
---
has_nvidia_gpu: true
```

### ë°©ë²• 2: Group Variables

GPU ë…¸ë“œ ê·¸ë£¹ì„ ë§Œë“¤ì–´ ê´€ë¦¬:

**inventory.ini:**
```ini
[gpu_nodes]
worker2 ansible_host=192.168.1.22
worker3 ansible_host=192.168.1.23

[gpu_nodes:vars]
has_nvidia_gpu=true
```

### ë°©ë²• 3: Extra Variables

ì‹¤í–‰ ì‹œ ë³€ìˆ˜ë¡œ ì§€ì •:

```bash
ansible-playbook -i inventory.ini site.yml \
  -l worker2 \
  -e "has_nvidia_gpu=true" \
  -e "install_gpu_driver=true"
```

### ë°©ë²• 4: Group Vars

`group_vars/gpu_nodes.yml` ìƒì„±:

```yaml
---
has_nvidia_gpu: true
install_gpu_driver: true
driver_version: "535.129.03"
```

---

## Containerd ì„¤ì • í…œí”Œë¦¿

### NVIDIA GPU Runtime Template

`roles/install_containerd/templates/containerd_nvidia.j2`ì—ì„œ ë‹¤ìŒ ì„¤ì •ì´ ì ìš©ë©ë‹ˆë‹¤:

```toml
[plugins."io.containerd.grpc.v1.cri".containerd]
  default_runtime_name = "nvidia"

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
  runtime_type = "io.containerd.runc.v2"
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
    BinaryName = "/usr/bin/nvidia-container-runtime"
    SystemdCgroup = true
```

### í‘œì¤€ Runtime Template

`roles/install_containerd/templates/containerd_config.toml.j2`ì—ì„œ ê¸°ë³¸ runc ì‚¬ìš©

---

## ê´€ë ¨ íŒŒì¼

| íŒŒì¼ | ì—­í•  | ì„¤ëª… |
|------|------|------|
| `roles/install_os_package/tasks/main.yml` | GPU ê°ì§€ | lspcië¡œ NVIDIA GPU ê°ì§€ ë° fact ì„¤ì • |
| `roles/install_containerd/tasks/main.yml` | Containerd ì„¤ì • | GPU fact ê¸°ë°˜ìœ¼ë¡œ config.toml ìƒì„± |
| `roles/install_containerd/templates/containerd_nvidia.j2` | NVIDIA í…œí”Œë¦¿ | NVIDIA runtime ì„¤ì • |
| `roles/install_containerd/templates/containerd_config.toml.j2` | í‘œì¤€ í…œí”Œë¦¿ | ê¸°ë³¸ runc ì„¤ì • |

---

## Kubernetesì—ì„œ GPU ì‚¬ìš©

Containerdê°€ NVIDIA ëŸ°íƒ€ìž„ìœ¼ë¡œ ì„¤ì •ë˜ë©´ Kubernetesì—ì„œ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

### 1. NVIDIA Device Plugin ì„¤ì¹˜ (ë³„ë„)

```bash
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.0/nvidia-device-plugin.yml
```

### 2. GPU Pod ë°°í¬ í…ŒìŠ¤íŠ¸

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  containers:
  - name: cuda
    image: nvidia/cuda:11.0-base
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
    # GPU ë…¸ë“œì—ë§Œ ë°°í¬
    kubernetes.io/hostname: worker2
```

```bash
kubectl apply -f gpu-test.yaml
kubectl logs gpu-test
```

---

## ì°¸ê³  ìžë£Œ

- [NVIDIA Container Toolkit ê³µì‹ ë¬¸ì„œ](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
- [Containerd Configuration](https://github.com/containerd/containerd/blob/main/docs/cri/config.md)
- [Kubernetes GPU Support](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-25
