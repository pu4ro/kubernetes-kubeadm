---
# =============================================================================
# KUBERNETES CLUSTER INITIALIZATION ROLE
# =============================================================================
# Initializes Kubernetes master node and configures worker nodes to join cluster
# Handles cluster setup with kubeadm and manages cluster access configuration

# -----------------------------------------------------------------------------
# KUBERNETES MASTER NODE INITIALIZATION
# -----------------------------------------------------------------------------
# Initialize the Kubernetes control plane on master nodes
# Sets up cluster with proper networking and configuration

- name: Initialize the Kubernetes master
  block:
    # Check if Kubernetes cluster is already initialized
    # Prevents re-initialization of existing clusters
    - name: Check if Kubernetes cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: k8s_admin_conf

    # Verify cluster health if already initialized
    # Checks if existing cluster is functional
    - name: Check if any Kubernetes nodes are already set up
      command: kubectl get nodes
      register: kubectl_nodes
      ignore_errors: true
      changed_when: false
      when: k8s_admin_conf.stat.exists

    # Generate kubeadm configuration file from template
    # Contains cluster-specific settings (networking, versions, etc.)
    - name: Template kubeadm config file
      template:
        src: kubeadm-init.yaml.j2
        dest: /root/kubeadm-init.yml

    # Initialize Kubernetes cluster using kubeadm
    # Only runs on master nodes that haven't been initialized
    # Failed conditions handle cases where cluster already exists
    - name: Initialize Kubernetes cluster with kubeadm
      command: kubeadm init --config=/root/kubeadm-init.yml
      when: 
        - inventory_hostname in groups['masters']
        - not k8s_admin_conf.stat.exists
      register: kubeadm_init_result
      failed_when: kubeadm_init_result.rc != 0 and "already exists" not in kubeadm_init_result.stderr 

    # -----------------------------------------------------------------------------
    # KUBECTL ACCESS CONFIGURATION
    # -----------------------------------------------------------------------------
    # Configure kubectl access for cluster administration
    # Sets up kubeconfig for command-line cluster management

    # Create .kube directory for kubectl configuration
    # Required directory for storing kubeconfig files
    - name: Create .kube directory
      file:
        path: "{{ ansible_env.HOME }}/.kube"
        state: directory
        mode: 0755
      when: inventory_hostname in groups['masters']

    # Copy cluster admin configuration to user's home directory
    # Enables kubectl access for cluster administration
    # backup: yes - preserves existing config if present
    - name: Copy admin kubeconfig to user's home directory
      copy:
        src: "/etc/kubernetes/admin.conf"
        dest: "{{ ansible_env.HOME }}/.kube/config"
        remote_src: yes
        mode: 0644
        backup: yes
      when: 
        - inventory_hostname in groups['masters']
        - k8s_admin_conf.stat.exists

    # Set proper ownership for kubeconfig file
    # Ensures current user can access kubectl commands
    - name: Change ownership of the kubeconfig
      file:
        path: "{{ ansible_env.HOME }}/.kube/config"
        owner: "{{ ansible_env.USER }}"
        group: "{{ ansible_env.USER }}"
        mode: 0644
      when: 
        - inventory_hostname in groups['masters']
        - k8s_admin_conf.stat.exists 

    # -----------------------------------------------------------------------------
    # WORKER NODE JOIN TOKEN GENERATION
    # -----------------------------------------------------------------------------
    # Generate join command for worker nodes to join the cluster
    # Creates secure token-based join command

    # Generate kubeadm join command with fresh token
    # Retries on failure to handle temporary API server unavailability
    - name: Get kubeadm join command
      command: kubeadm token create --print-join-command
      register: join_command
      changed_when: false
      when: 
        - inventory_hostname in groups['masters']
        - k8s_admin_conf.stat.exists
      retries: 3
      delay: 5

    # Store join command as Ansible fact for worker nodes
    # Makes join command available to all hosts in the play
    # run_once: Only needs to be generated once per cluster
    - name: Set join command as fact
      set_fact:
        join_cmd: "{{ join_command.stdout }}"
      when: 
        - inventory_hostname in groups['masters']
        - join_command is defined
        - join_command.stdout is defined
      delegate_to: "{{ groups['masters'][0] }}"
      run_once: true 

# -----------------------------------------------------------------------------
# WORKER NODE CLUSTER JOIN
# -----------------------------------------------------------------------------
# Configure worker nodes to join the Kubernetes cluster
# Uses join command generated by master node

# Check if worker node is already part of the cluster
# Prevents unnecessary re-joining of existing cluster members
- name: Check if the node is already part of the cluster
  stat:
    path: /etc/kubernetes/kubelet.conf
  register: node_already_joined
  when: inventory_hostname in groups['workers']

# Join worker nodes to the Kubernetes cluster
# Uses join command from master node with retry logic
# Handles cases where node is already a cluster member
- name: Join the Kubernetes worker nodes
  command: "{{ hostvars[groups['masters'][0]]['join_cmd'] }}"
  when: 
    - inventory_hostname in groups['workers'] 
    - not node_already_joined.stat.exists
    - hostvars[groups['masters'][0]]['join_cmd'] is defined
  register: join_result
  retries: 3
  delay: 10
  until: join_result.rc == 0
  failed_when: join_result.rc != 0 and "already a member" not in join_result.stderr

