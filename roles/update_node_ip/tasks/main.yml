---
# Update Node IP Role - Main Tasks
# This role updates Kubernetes configuration files when a node's IP address changes
# Note: This role only updates files. User must handle IP change on the system.

- name: Set new_ip from ansible_host if not defined
  set_fact:
    new_ip: "{{ new_ip | default(ansible_host) }}"

- name: Check if this is a master node
  stat:
    path: "{{ k8s_manifests_dir }}/kube-apiserver.yaml"
  register: is_master_node

# Stop kubelet first to prevent issues during file updates
- name: Stop kubelet before updating files
  systemd:
    name: kubelet
    state: stopped
  when: restart_services | default(true)

# Remove all containers to clear kubelet cache
- name: Remove all containers (crictl rm)
  shell: crictl rm -af 2>/dev/null || true
  when: restart_services | default(true)

- name: Remove all pods (crictl rmp)
  shell: crictl rmp -af 2>/dev/null || true
  when: restart_services | default(true)

# Update manifest files
- name: Update IP in Kubernetes manifest files
  replace:
    path: "{{ k8s_manifests_dir }}/{{ item }}"
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  loop: "{{ k8s_manifest_files }}"
  when: is_master_node.stat.exists
  ignore_errors: yes

# Update kubeconfig files
- name: Update IP in kubeconfig files
  replace:
    path: "{{ k8s_config_dir }}/{{ item }}"
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  loop: "{{ k8s_kubeconfig_files }}"
  when: is_master_node.stat.exists
  ignore_errors: yes

# Update user's kubeconfig
- name: Update IP in user's kubeconfig (~/.kube/config)
  replace:
    path: "{{ ansible_env.HOME }}/.kube/config"
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  ignore_errors: yes

# Update kubelet config
- name: Update IP in kubelet config
  replace:
    path: /var/lib/kubelet/config.yaml
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  ignore_errors: yes

# Update /etc/hosts
- name: Update IP in /etc/hosts
  replace:
    path: /etc/hosts
    regexp: "^{{ old_ip }}(\\s+)"
    replace: "{{ new_ip }}\\1"
    backup: no
  when: update_etc_hosts | default(true)

# Remove backup files from manifests directory
- name: Remove backup files from manifests directory
  shell: rm -f /etc/kubernetes/manifests/*~
  ignore_errors: yes

# Regenerate certificates if required
- name: Regenerate certificates block
  block:
    - name: Backup existing certificates
      archive:
        path: /etc/kubernetes/pki
        dest: "/etc/kubernetes/pki-backup-{{ ansible_date_time.iso8601_basic_short }}.tar.gz"
        format: gz

    - name: Remove old etcd certificates
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes/pki/etcd/server.crt
        - /etc/kubernetes/pki/etcd/server.key
        - /etc/kubernetes/pki/etcd/peer.crt
        - /etc/kubernetes/pki/etcd/peer.key

    - name: Remove old apiserver certificates
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes/pki/apiserver.crt
        - /etc/kubernetes/pki/apiserver.key
        - /etc/kubernetes/pki/apiserver-etcd-client.crt
        - /etc/kubernetes/pki/apiserver-etcd-client.key
        - /etc/kubernetes/pki/apiserver-kubelet-client.crt
        - /etc/kubernetes/pki/apiserver-kubelet-client.key

    # Build extra SANs for certificates
    - name: Build certificate SANs list
      set_fact:
        cert_extra_sans: >-
          {{ new_ip }},127.0.0.1,localhost,{{ inventory_hostname }}
          {%- if api_domain is defined -%},{{ api_domain }}{%- endif -%}
          {%- if enable_domain_communication | default(false) and domain_suffix is defined -%},{{ inventory_hostname }}.{{ domain_suffix }}{%- endif -%}
          {%- if kube_vip_address is defined -%},{{ kube_vip_address }}{%- endif -%}

    # Build etcd SANs list for domain communication
    - name: Build etcd SANs list
      set_fact:
        etcd_extra_sans: >-
          {{ new_ip }},127.0.0.1,localhost,{{ inventory_hostname }}
          {%- if enable_domain_communication | default(false) and domain_suffix is defined -%},{{ inventory_hostname }}.{{ domain_suffix }}{%- endif -%}

    # Create kubeadm config for certificate regeneration with etcd SANs
    - name: Create kubeadm config for cert regeneration
      copy:
        content: |
          apiVersion: kubeadm.k8s.io/v1beta3
          kind: ClusterConfiguration
          etcd:
            local:
              serverCertSANs:
                - "{{ new_ip }}"
                - "127.0.0.1"
                - "localhost"
                - "{{ inventory_hostname }}"
          {% if enable_domain_communication | default(false) and domain_suffix is defined %}
                - "{{ inventory_hostname }}.{{ domain_suffix }}"
          {% endif %}
              peerCertSANs:
                - "{{ new_ip }}"
                - "127.0.0.1"
                - "localhost"
                - "{{ inventory_hostname }}"
          {% if enable_domain_communication | default(false) and domain_suffix is defined %}
                - "{{ inventory_hostname }}.{{ domain_suffix }}"
          {% endif %}
        dest: /tmp/kubeadm-cert-config.yaml
        mode: '0600'

    # Regenerate etcd certificates with domain SANs
    - name: Regenerate etcd server certificates with SANs
      command: kubeadm init phase certs etcd-server --config /tmp/kubeadm-cert-config.yaml

    - name: Regenerate etcd peer certificates with SANs
      command: kubeadm init phase certs etcd-peer --config /tmp/kubeadm-cert-config.yaml

    # Regenerate apiserver certificates with extra SANs
    - name: Regenerate apiserver certificates with domain SANs
      command: >
        kubeadm init phase certs apiserver
        --apiserver-advertise-address={{ new_ip }}
        --apiserver-cert-extra-sans={{ cert_extra_sans }}

    - name: Regenerate apiserver-etcd-client certificates
      command: kubeadm init phase certs apiserver-etcd-client

    - name: Regenerate apiserver-kubelet-client certificates
      command: kubeadm init phase certs apiserver-kubelet-client

    # Cleanup temp config
    - name: Remove temporary kubeadm config
      file:
        path: /tmp/kubeadm-cert-config.yaml
        state: absent

  when:
    - regenerate_certs | default(false)
    - is_master_node.stat.exists

# Force new cluster for etcd (preserves data but resets cluster membership)
# This is required when IP changes because etcd stores cluster info in its data directory
- name: Force new etcd cluster block
  block:
    - name: Check if etcd data exists
      stat:
        path: /var/lib/etcd/member
      register: etcd_member_dir

    - name: Add --force-new-cluster flag to etcd manifest
      replace:
        path: "{{ k8s_manifests_dir }}/etcd.yaml"
        regexp: '(- etcd\n)(    - --advertise-client-urls)'
        replace: '\1    - --force-new-cluster\n\2'
        backup: no
      when: etcd_member_dir.stat.exists

    - name: Start kubelet to bootstrap etcd with force-new-cluster
      systemd:
        name: kubelet
        state: started
      when: etcd_member_dir.stat.exists

    - name: Wait for etcd to start with new cluster
      pause:
        seconds: 30
      when: etcd_member_dir.stat.exists

    - name: Wait for etcd container to be running
      shell: |
        for i in $(seq 1 30); do
          if crictl ps 2>/dev/null | grep -q etcd; then
            echo "etcd container is running"
            exit 0
          fi
          sleep 2
        done
        echo "etcd container not found after 60 seconds"
        exit 1
      register: etcd_check
      failed_when: false
      when: etcd_member_dir.stat.exists

    - name: Verify etcd health
      shell: |
        crictl exec $(crictl ps -q --name etcd 2>/dev/null | head -1) etcdctl \
          --endpoints=https://127.0.0.1:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key \
          endpoint health 2>&1 || echo "etcd health check failed"
      register: etcd_health
      failed_when: false
      when: etcd_member_dir.stat.exists

    - name: Display etcd health status
      debug:
        msg: "{{ etcd_health.stdout | default('etcd health check skipped') }}"
      when: etcd_member_dir.stat.exists

    - name: Stop kubelet to remove force-new-cluster flag
      systemd:
        name: kubelet
        state: stopped
      when: etcd_member_dir.stat.exists

    - name: Remove all containers again
      shell: crictl rm -af 2>/dev/null || true
      when: etcd_member_dir.stat.exists

    - name: Remove all pods again
      shell: crictl rmp -af 2>/dev/null || true
      when: etcd_member_dir.stat.exists

    - name: Remove --force-new-cluster flag from etcd manifest
      replace:
        path: "{{ k8s_manifests_dir }}/etcd.yaml"
        regexp: '    - --force-new-cluster\n'
        replace: ''
        backup: no
      when: etcd_member_dir.stat.exists

  when:
    - force_new_etcd_cluster | default(true)
    - is_master_node.stat.exists
    - restart_services | default(true)

# Reset etcd data directory if required (WARNING: This will lose all etcd data!)
- name: Reset etcd data block
  block:
    - name: Check if etcd data directory exists
      stat:
        path: /var/lib/etcd/member
      register: etcd_data_dir

    - name: Backup etcd data directory
      archive:
        path: /var/lib/etcd
        dest: "/var/lib/etcd-backup-{{ ansible_date_time.iso8601_basic_short }}.tar.gz"
        format: gz
      when: etcd_data_dir.stat.exists

    - name: Remove etcd data directory
      file:
        path: /var/lib/etcd
        state: absent
      when: etcd_data_dir.stat.exists

    - name: Recreate empty etcd data directory
      file:
        path: /var/lib/etcd
        state: directory
        mode: '0700'

  when:
    - reset_etcd_data | default(false)
    - is_master_node.stat.exists

# Start services
- name: Start kubelet
  systemd:
    name: kubelet
    state: started
  when: restart_services | default(true)

- name: Wait for control plane pods to start
  pause:
    seconds: 30
  when:
    - restart_services | default(true)
    - is_master_node.stat.exists
