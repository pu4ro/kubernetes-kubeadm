---
# Update Node IP Role - Main Tasks
# This role updates Kubernetes configuration files when a node's IP address changes
# Note: This role only updates files. User must handle IP change on the system.

- name: Set new_ip from ansible_host if not defined
  set_fact:
    new_ip: "{{ new_ip | default(ansible_host) }}"

- name: Check if this is a master node
  stat:
    path: "{{ k8s_manifests_dir }}/kube-apiserver.yaml"
  register: is_master_node

# Stop kubelet first to prevent issues during file updates
- name: Stop kubelet before updating files
  systemd:
    name: kubelet
    state: stopped
  when: restart_services | default(true)

# Remove all containers to clear kubelet cache
- name: Remove all containers (crictl rm)
  shell: crictl rm -af 2>/dev/null || true
  when: restart_services | default(true)

- name: Remove all pods (crictl rmp)
  shell: crictl rmp -af 2>/dev/null || true
  when: restart_services | default(true)

# Update manifest files
- name: Update IP in Kubernetes manifest files
  replace:
    path: "{{ k8s_manifests_dir }}/{{ item }}"
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  loop: "{{ k8s_manifest_files }}"
  when: is_master_node.stat.exists
  ignore_errors: yes

# Update kubeconfig files
- name: Update IP in kubeconfig files
  replace:
    path: "{{ k8s_config_dir }}/{{ item }}"
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  loop: "{{ k8s_kubeconfig_files }}"
  when: is_master_node.stat.exists
  ignore_errors: yes

# Update user's kubeconfig
- name: Update IP in user's kubeconfig (~/.kube/config)
  replace:
    path: "{{ ansible_env.HOME }}/.kube/config"
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  ignore_errors: yes

# Update kubelet config
- name: Update IP in kubelet config
  replace:
    path: /var/lib/kubelet/config.yaml
    regexp: "{{ old_ip }}"
    replace: "{{ new_ip }}"
    backup: no
  ignore_errors: yes

# Update /etc/hosts
- name: Update IP in /etc/hosts
  replace:
    path: /etc/hosts
    regexp: "^{{ old_ip }}(\\s+)"
    replace: "{{ new_ip }}\\1"
    backup: no
  when: update_etc_hosts | default(true)

# Remove backup files from manifests directory
- name: Remove backup files from manifests directory
  shell: rm -f /etc/kubernetes/manifests/*~
  ignore_errors: yes

# Regenerate certificates if required
- name: Regenerate certificates block
  block:
    - name: Backup existing certificates
      archive:
        path: /etc/kubernetes/pki
        dest: "/etc/kubernetes/pki-backup-{{ ansible_date_time.iso8601_basic_short }}.tar.gz"
        format: gz

    - name: Remove old etcd certificates
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes/pki/etcd/server.crt
        - /etc/kubernetes/pki/etcd/server.key
        - /etc/kubernetes/pki/etcd/peer.crt
        - /etc/kubernetes/pki/etcd/peer.key

    - name: Remove old apiserver certificates
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes/pki/apiserver.crt
        - /etc/kubernetes/pki/apiserver.key
        - /etc/kubernetes/pki/apiserver-etcd-client.crt
        - /etc/kubernetes/pki/apiserver-etcd-client.key
        - /etc/kubernetes/pki/apiserver-kubelet-client.crt
        - /etc/kubernetes/pki/apiserver-kubelet-client.key

    # Build extra SANs for certificates
    - name: Build certificate SANs list
      set_fact:
        cert_extra_sans: >-
          {{ new_ip }},127.0.0.1,localhost,{{ inventory_hostname }}
          {%- if api_domain is defined -%},{{ api_domain }}{%- endif -%}
          {%- if enable_domain_communication | default(false) and domain_suffix is defined -%},{{ inventory_hostname }}.{{ domain_suffix }}{%- endif -%}
          {%- if kube_vip_address is defined -%},{{ kube_vip_address }}{%- endif -%}

    # Regenerate etcd certificates
    - name: Regenerate etcd server certificates
      command: kubeadm init phase certs etcd-server

    - name: Regenerate etcd peer certificates
      command: kubeadm init phase certs etcd-peer

    # Regenerate apiserver certificates with extra SANs
    - name: Regenerate apiserver certificates with domain SANs
      command: >
        kubeadm init phase certs apiserver
        --apiserver-advertise-address={{ new_ip }}
        --apiserver-cert-extra-sans={{ cert_extra_sans }}

    - name: Regenerate apiserver-etcd-client certificates
      command: kubeadm init phase certs apiserver-etcd-client

    - name: Regenerate apiserver-kubelet-client certificates
      command: kubeadm init phase certs apiserver-kubelet-client

  when:
    - regenerate_certs | default(false)
    - is_master_node.stat.exists

# Force new cluster for etcd (preserves data but resets cluster membership)
# This is required when IP changes because etcd stores cluster info in its data directory
- name: Force new etcd cluster block
  block:
    - name: Check if etcd data exists
      stat:
        path: /var/lib/etcd/member
      register: etcd_member_dir

    - name: Add --force-new-cluster flag to etcd manifest
      lineinfile:
        path: "{{ k8s_manifests_dir }}/etcd.yaml"
        insertafter: "- etcd"
        line: "    - --force-new-cluster"
        state: present
      when: etcd_member_dir.stat.exists

    - name: Start kubelet to bootstrap etcd with force-new-cluster
      systemd:
        name: kubelet
        state: started
      when: etcd_member_dir.stat.exists

    - name: Wait for etcd to start with new cluster
      pause:
        seconds: 20
      when: etcd_member_dir.stat.exists

    - name: Stop kubelet to remove force-new-cluster flag
      systemd:
        name: kubelet
        state: stopped
      when: etcd_member_dir.stat.exists

    - name: Remove all containers again
      shell: crictl rm -af 2>/dev/null || true
      when: etcd_member_dir.stat.exists

    - name: Remove all pods again
      shell: crictl rmp -af 2>/dev/null || true
      when: etcd_member_dir.stat.exists

    - name: Remove --force-new-cluster flag from etcd manifest
      lineinfile:
        path: "{{ k8s_manifests_dir }}/etcd.yaml"
        line: "    - --force-new-cluster"
        state: absent
      when: etcd_member_dir.stat.exists

  when:
    - force_new_etcd_cluster | default(true)
    - is_master_node.stat.exists
    - restart_services | default(true)

# Reset etcd data directory if required (WARNING: This will lose all etcd data!)
- name: Reset etcd data block
  block:
    - name: Check if etcd data directory exists
      stat:
        path: /var/lib/etcd/member
      register: etcd_data_dir

    - name: Backup etcd data directory
      archive:
        path: /var/lib/etcd
        dest: "/var/lib/etcd-backup-{{ ansible_date_time.iso8601_basic_short }}.tar.gz"
        format: gz
      when: etcd_data_dir.stat.exists

    - name: Remove etcd data directory
      file:
        path: /var/lib/etcd
        state: absent
      when: etcd_data_dir.stat.exists

    - name: Recreate empty etcd data directory
      file:
        path: /var/lib/etcd
        state: directory
        mode: '0700'

  when:
    - reset_etcd_data | default(false)
    - is_master_node.stat.exists

# Start services
- name: Start kubelet
  systemd:
    name: kubelet
    state: started
  when: restart_services | default(true)

- name: Wait for control plane pods to start
  pause:
    seconds: 30
  when:
    - restart_services | default(true)
    - is_master_node.stat.exists
