---
# Basic Kubernetes Cluster Configuration

# Kubernetes Version Configuration
# Supported Kubernetes versions:
#   - k8s 1.27.x and below: Uses kubeadm v1beta3 API
#   - k8s 1.28.x - 1.30.x: Can use v1beta3 or v1beta4 API
#   - k8s 1.31.x and above: Requires kubeadm v1beta4 API (configured in templates)
#
# This playbook uses v1beta4 API format and is optimized for k8s 1.31+
# Key changes in 1.31+:
#   - kubeadm config: v1beta3 â†’ v1beta4 (InitConfiguration + ClusterConfiguration)
#   - kube-vip: Uses super-admin.conf instead of admin.conf
#   - API server extraArgs: Changed from map to list format (name/value pairs)
#
kubernetes_version: '1.34.1'        # Recommended: 1.31.x or higher for full feature support
dns_domain: cluster.local
service_subnet: 10.96.0.0/12
pod_subnet: 10.244.0.0/16

# High Availability Settings (for multi-master setup)
master_ha: true
kube_vip_port: 6443
kube_vip_interface: ens18
kube_vip_address: 192.168.135.69  # Uncomment for HA setup
kube_vip_svc_enable: false         # Enable/disable kube-vip service load balancing (true/false)

# Container Runtime Configuration
# Supported containerd versions:
#   - containerd 1.6.x - 1.7.x: Config version 2 format (deprecated but supported)
#   - containerd 2.0.x - 2.1.x: Config version 2 format with new plugin structure
#   - containerd 2.2.x+: Config version 3 format (new pinned_images structure)
#
# The playbook automatically detects the installed containerd version and applies
# the appropriate configuration format. Both old and new formats are supported.
#
# Configuration differences by version:
#   < 2.2: Uses sandbox_image for pause container, old plugin paths
#   >= 2.2: Uses pinned_images.sandbox, new io.containerd.cri.v1.* plugins
#
containerd_version: "1.7.6"         # Reference only (auto-detected at runtime)
containerd_limit_nproc: 100000      # systemd LimitNPROC for containerd.service
containerd_limit_nofile: 100000     # systemd LimitNOFILE for containerd.service
enable_local_registry: false
local_registry_image: "registry:2"
local_registry_image_tar: "/root/docker.tar.gz"
local_registry_container_name: "local-registry"
local_registry_host_port: 80
local_registry_container_port: 5000
local_registry_data_dir: "/opt/local-registry/data"
local_registry_additional_args: []

# Containerd Data Directory Configuration
# Set custom data directory for containerd (default: /var/lib/containerd)
# The actual path will be: {containerd_data_base_dir}/{hostname}
# Example: /data/containerd/worker1, /data/containerd/worker2
# Leave empty or comment out to use default /var/lib/containerd
containerd_data_base_dir: ""  # Base directory for containerd data
# containerd_data_base_dir: ""  # Use this to disable custom path and use default

# System Configuration
set_timezone: Asia/Seoul
configure_etc_hosts: true
set_hostname_from_inventory: true

# Sysctl Parameters for Kubernetes
# These settings are applied via configure_sysctl role
# Modify these values to customize kernel parameters for your environment
k8s_sysctl_params:
  # Enable IP forwarding
  net.ipv4.ip_forward: 1

  # Enable bridge netfilter
  net.bridge.bridge-nf-call-iptables: 1
  net.bridge.bridge-nf-call-ip6tables: 1

  # Disable swap
  vm.swappiness: 0

  # Maximum number of open files
  fs.file-max: 2097152

  # Maximum number of inotify watches
  fs.inotify.max_user_watches: 524288
  fs.inotify.max_user_instances: 512

  # Connection tracking
  net.netfilter.nf_conntrack_max: 1048576

  # Network tuning
  net.ipv4.tcp_max_syn_backlog: 8096
  net.core.netdev_max_backlog: 16384
  net.core.somaxconn: 32768

  # Memory settings (important for Elasticsearch, OpenSearch, etc.)
  # Increase if you see "max virtual memory areas vm.max_map_count [65530] is too low" error
  vm.max_map_count: 262144

# NTP/Chrony Configuration
use_local_ntp: true                    # true: master1 as NTP server, true: use external NTP
external_ntp_servers: {}             # External NTP servers
  #  - "pool.ntp.org"
  #  - "time.google.com"
  #  - "time.cloudflare.com"
cluster_network: "192.168.0.0/16"     # Network range allowed to access local NTP server

# Cross-platform service and config paths
chrony_service_name: "{{ 'chronyd' if ansible_os_family == 'RedHat' else 'chrony' }}"
chrony_config_path: "{{ '/etc/chrony.conf' if ansible_os_family == 'RedHat' else '/etc/chrony/chrony.conf' }}"

# Network Plugin (flannel is default)
network_plugin: "flannel"

# Master Node Scheduling (allow pods on master for single-node setup)
allow_master_scheduling: true

# Certificate Extension (extend certificates to 10 years)
extend_k8s_certificates: true

# Parallel Execution Control
# Set how many hosts to run simultaneously during installation
# 1 = sequential (one at a time), 0 = all hosts in parallel
parallel_execution:
  system_preparation: 0     # How many hosts for system prep phase (packages, containerd)
  package_installation: 0   # How many hosts for apt/yum operations simultaneously
  kubernetes_installation: 0  # How many hosts for kubernetes installation (0 = all parallel)

# Example configurations:
# For fast installation on reliable network (all parallel):
# parallel_execution:
#   system_preparation: 0
#   package_installation: 0  
#   kubernetes_installation: 0
#
# For mixed approach (packages sequential, k8s parallel):
# parallel_execution:
#   system_preparation: 1
#   package_installation: 1
#   kubernetes_installation: 0
#
# For completely sequential installation:
# parallel_execution:
#   system_preparation: 1
#   package_installation: 1
#   kubernetes_installation: 1

# Package Repository URLs (for offline installation)
repo_url:
  centos: "http://192.168.1.10:8080/repo/"
  ubuntu: "http://192.168.134.102/ubuntu_22.04.5"

# Ubuntu APT Repository Configuration
# Enable this to use custom Ubuntu repository (local or HTTP)
# Example: Apache HTTP server serving local packages
enable_ubuntu_repo: false
ubuntu_repo_url: ""  # Example: "http://192.168.1.100:8080/ubuntu-repo"
ubuntu_repo_components: "./"
ubuntu_repo_trusted: true
ubuntu_repo_update_cache: true

# RHEL/CentOS YUM Repository Configuration (supports multiple repositories)
# Enable this to use custom RHEL repositories (local or HTTP)
# You can configure multiple repositories with different priorities
#
# Usage:
#   ansible-playbook site.yml --tags rhel-repo
#
enable_rhel_repos: true
rhel_repos:
  - name: "rhel-iso-repo"
    id: "rhel-iso-repo"
    url: "http://192.168.135.71:8080/rhel-repo"
    type: "baseos_appstream"  # Options: "single" or "baseos_appstream"
    enabled: 1
    gpgcheck: 0
    priority: 1
  # Example: Directory-based repository
  - name: "rhel-directory-repo"
    id: "rhel-directory-repo"
    url: "http://192.168.135.71:8080/rhel-repo2"
    type: "single"
    enabled: 1
    gpgcheck: 0
    priority: 2
  # Add more repositories as needed
  # - name: "custom-repo"
  #   id: "custom-repo"
  #   url: "http://192.168.135.72:8080/custom-repo/"
  #   type: "single"
  #   enabled: 1
  #   gpgcheck: 0
  #   priority: 3

# Container Registry Settings (basic Docker configuration)
insecure_registries: []
  #- "192.168.1.10:5000"           # Local registry example
  # - "harbor.yourdomain.com"     # Harbor registry example
  # - "registry.local"            # Custom registry example
docker_log_max_size: "100m"

# Containerd Registry Authentication Configuration
# Configure authentication for registries in containerd config.toml
# This complements docker_registries (used for nerdctl login)
#
# Usage:
#   ansible-playbook site.yml --tags containerd-config
#
containerd_registry_auth:
  # Example configurations:
  - registry: "registry.test"
    insecure_skip_verify: true
    username: ""          # Leave empty for no auth
    password: ""
    auth: ""
    identitytoken: ""
  - registry: "registry.local"
    insecure_skip_verify: true
    username: ""
    password: ""
    auth: ""
    identitytoken: ""
  # - registry: "cr.makina.rocks"
  #   insecure_skip_verify: false
  #   username: "your-username"
  #   password: "your-password"
  #   auth: ""              # Optional: base64 encoded username:password
  #   identitytoken: ""     # Optional: for token-based auth
  # - registry: "harbor.yourdomain.com"
  #   insecure_skip_verify: true
  #   username: "admin"
  #   password: "Harbor12345"
  #   auth: ""
  #   identitytoken: ""

# Kubernetes Image Repository Configuration
# Configure which image repository to use for Kubernetes components
# When disabled, uses the default registry.k8s.io
#
# Usage:
#   enable_custom_image_repository: true   - Use custom registry (internal/mirror)
#   enable_custom_image_repository: false  - Use default registry.k8s.io
#
enable_custom_image_repository: true                   # Enable/disable custom image repository
k8s_image_repository: "cr.makina.rocks/external-hub/kubernetes"           # Custom registry for k8s components
coredns_image_repository: "cr.makina.rocks/external-hub/kubernetes/coredns"  # Custom registry for CoreDNS

# Kubernetes Pause Container Image
# Pause container versions by Kubernetes version:
#   - k8s 1.26.x and below: pause:3.9
#   - k8s 1.27.x - 1.30.x: pause:3.9 or 3.10
#   - k8s 1.31.x and above: pause:3.10 or 3.10.1 (recommended)
#
# The pause container is the infrastructure container that holds network namespaces
# for all pods. It's configured differently based on containerd version:
#   - containerd < 2.2: Set via sandbox_image in config.toml
#   - containerd >= 2.2: Set via pinned_images.sandbox in config.toml (version 3 format)
#
pause_image: "cr.makina.rocks/external-hub/kubernetes/pause:3.10.1"

# NVIDIA GPU Driver Installation (Optional)
# Enable this to automatically install NVIDIA GPU drivers
# 
# Usage with tags:
#   ansible-playbook site.yml --tags install-nvidia-driver
#
enable_nvidia_driver_install: true
nvidia_driver_version: "570.124.06"
nvidia_driver_download_url: "http://192.168.135.71:8080/rhel-repo2"
nvidia_driver_filename: "NVIDIA-Linux-x86_64-{{ nvidia_driver_version }}.run"
nvidia_driver_install_options: "--silent --no-questions --ui=none --disable-nouveau"

# NVIDIA Toolkit Path Fix
# Fix NVIDIA toolkit path in containerd config from /usr/local/nvidia/toolkit to /usr/bin
# This is required when NVIDIA driver is installed via .run file (not package manager)
# The .run file installer places binaries in /usr/local/nvidia/toolkit, but containerd
# may need them in /usr/bin for proper GPU support
#
# Usage with tags:
#   ansible-playbook site.yml --tags fix-nvidia-toolkit-path
#
# Or with Makefile:
#   make fix-nvidia-toolkit-path
#
fix_nvidia_toolkit_path: false  # Enable to fix NVIDIA toolkit path (needed for .run file installs)

# GPU Operator Configuration (optional, for Kubernetes GPU support)
enable_gpu_operator: false
gpu_operator_namespace: "gpu-operator"
gpu_operator_chart_path: "./gpu-operator"
gpu_operator_driver_enabled: false  # Set to false when installing driver manually

# NVIDIA Containerd Runtime Configuration
# Enable NVIDIA runtime support in containerd configuration
# When enabled, containerd will be configured with NVIDIA runtime if GPU is detected
# GPU detection is automatic via lspci, but this setting controls whether to apply the configuration
# Set to false to disable NVIDIA runtime configuration even if GPU is detected
enable_nvidia_containerd_config: false  # Enable/disable NVIDIA runtime in containerd config

# GPU Node Labeling
# Automatically label nodes with 'gpu=on' if NVIDIA GPU is detected
# This allows easy pod scheduling to GPU nodes using nodeSelector
# 
# Usage with tags:
#   ansible-playbook site.yml --tags label-gpu-nodes    # Label GPU nodes
#
enable_gpu_node_labels: true            # Enable/disable automatic GPU node labeling

# Docker Registry Credentials Configuration
# Set docker_login_required to true to enable registry authentication
# Supports multiple registries with different protocols (http/https)
# 
# Usage with tags:
#   ansible-playbook site.yml --tags docker-credentials        # Full setup
#   ansible-playbook site.yml --tags nerdctl-login            # Login only
#   ansible-playbook site.yml --tags containerd-config        # Config only
#   ansible-playbook site.yml --tags restart-kubelet          # Restart only
#
docker_login_required: true
docker_registries:
  # Example configuration for multiple registries:
  - registry: "cr.makina.rocks"
    protocol: "https"  # https (default) or http
    username: "mrx.dev"
    password: "GMflzHdAeR3KhGgXoecXR3lTKO0lSX5O"
  - registry: "harbor.runway.test"
    protocol: "http"   # for insecure registries
    username: "admin" 
    password: "Harbor12345"
  # - registry: "gcr.io"
  #   protocol: "https"
  #   username: "_json_key"
  #   password: "{{ gcp_service_account_key }}"

# Registry Host Mapping (for /etc/hosts configuration)
# Map registry domains to their IP addresses
# This will be added to /etc/hosts along with inventory host information
# 
# Usage with tags:
#   ansible-playbook site.yml --tags hosts-config    # Update /etc/hosts only
#   ansible-playbook site.yml --tags common          # Full common setup
#
registry_hosts:
  "harbor.runway.test": "192.168.135.28"
  # Add more registry domains and their IPs as needed

# Custom Host Mappings (for /etc/hosts configuration)
# Add any custom hostname-to-IP mappings that you need
# These will be added to /etc/hosts on all nodes
#
# Example:
#   custom_hosts:
#     "myapp.example.com": "10.0.0.100"
#     "db.example.com": "10.0.0.101"
#     "api.example.com": "10.0.0.102"
#
custom_hosts: {}
  # Add your custom host mappings here

# CoreDNS Hosts Configuration (add registry hosts to CoreDNS)
configure_coredns_hosts: true

# OIDC Authentication Configuration for Kube-APIServer
# Enable OIDC authentication for Kubernetes API server
# This configures the API server to authenticate users via an external OIDC provider (e.g., Keycloak)
#
# Usage with tags:
#   ansible-playbook site.yml --tags oidc-apiserver    # Configure OIDC only
#
enable_oidc_apiserver: true                           # Enable/disable OIDC configuration
domain_host: "runway.test"                             # Base domain for OIDC issuer URL

# OIDC Provider Configuration
oidc_client_id: "kubernetes"                           # OIDC client ID registered in the provider
oidc_username_claim: "preferred_username"              # JWT claim to use as username
oidc_groups_claim: "client_roles"                      # JWT claim to use for group membership
oidc_issuer_url: "https://keycloak.{{ domain_host }}/realms/runway"  # OIDC issuer URL

# Additional OIDC settings (optional)
# oidc_username_prefix: ""                             # Prefix for usernames (e.g., "oidc:")
# oidc_groups_prefix: ""                               # Prefix for groups (e.g., "oidc:")
# oidc_ca_file: ""                                     # Path to CA file for OIDC issuer (if self-signed)
# oidc_required_claim: ""                              # Additional required claims (e.g., "groups=admins")

# PodNodeSelector Admission Plugin
# Enable PodNodeSelector admission plugin to restrict pod placement
enable_pod_node_selector: true                        # Enable/disable PodNodeSelector plugin

# Containerd Registry Mirror Configuration
# IMPORTANT: This configuration is REQUIRED for Runway 2.0+ environments
# Configure containerd to use internal registry mirrors for external registries
# This allows pulling images from internal registry instead of external sources
#
# Usage with tags:
#   ansible-playbook site.yml --tags registry-mirror    # Configure mirrors
#
enable_registry_mirror: true                          # Enable/disable registry mirror configuration
registry_mirror_host: "cr.makina.rocks"                # Internal registry host
registry_mirror_user: "mrx.dev"                        # Registry username
registry_mirror_password: "GMflzHdAeR3KhGgXoecXR3lTKO0lSX5O"  # Registry password
registry_mirror_path_prefix: ""                        # Optional path prefix (e.g., "myproject")

# Registry Mappings (External Registry -> Internal Repository)
# Maps external registry domains to internal repository paths
# Format: "external.registry.io": "internal-repo-name"
registry_mirror_mappings:
  "docker.io": "docker-io"
  "cr.fluentbit.io": "cr-fluentbit-io"
  "docker.getcollate.io": "docker-getcollate-io"
  "docker.gitea.com": "docker-gitea-com"
  "ecr-public.aws.com": "ecr-public-aws-com"
  "ghcr.io": "ghcr-io"
  "nvcr.io": "nvcr-io"
  "quay.io": "quay-io"
  "registry.cn-hangzhou.aliyuncs.com": "registry-cn-hangzhou-aliyuncs-com"
  "registry.k8s.io": "registry-k8s-io"

# OS Package Installation Configuration
# Configure packages to install for Ubuntu/Debian and RHEL/CentOS systems
# These lists are used by the install_os_package role

# Ubuntu/Debian Packages
ubuntu_packages:
  - kubelet
  - kubeadm
  - kubectl
  - bash-completion
  - dnsmasq
  - jq
  - openssh-server
  - ca-certificates
  - nfs-common
  - nfs-kernel-server
  - net-tools
  - containerd
  - chrony
  - curl
  - wget
  - k9s

# Ubuntu/Debian Optional Packages (installed when enabled)
# Enable with: enable_nerdctl_buildkit: true
enable_nerdctl_buildkit: false  # Enable/disable nerdctl and buildkit installation
ubuntu_optional_packages:
  - nerdctl
  - buildkit

# Helmfile Installation (optional, master nodes only)
# Enable with: enable_helmfile: true
enable_helmfile: false  # Enable/disable helmfile installation

# Ubuntu/Debian Master-only Packages (installed on 'installs' group)
ubuntu_master_only_packages:
  - make

# RHEL/CentOS Required Packages
rhel_packages:
  - kubelet
  - kubeadm
  - kubectl
  - bash-completion
  - dnsmasq
  - jq
  - openssh-server
  - ca-certificates
  - tree
  - device-mapper-persistent-data
  - lvm2
  - chrony
  - curl
  - wget
  - which
  - k9s

# RHEL/CentOS 8 Specific Packages (version-pinned)
rhel8_packages:
  - containerd.io-1.6.32-3.1.el8

# RHEL/CentOS 9 Specific Packages
rhel9_packages:
  - containerd.io

# RHEL/CentOS Optional Packages (install if available, errors ignored)
rhel_optional_packages:
  - nfs-utils
  - yum-utils

# RHEL/CentOS Master-only Packages (installed on 'installs' group)
rhel_master_only_packages:
  - make

# Package Name Mappings (for distribution-specific package names)
# Used when package names differ between distributions
package_name_map:
  nfs_client:
    Debian: nfs-common
    RedHat: nfs-utils
  nfs_server:
    Debian: nfs-kernel-server
    RedHat: nfs-utils
  network_tools:
    Debian: net-tools
    RedHat: net-tools
  time_sync:
    Debian: chrony
    RedHat: chrony
  helmfile:
    Debian: helmfile
    RedHat: helmfile-bundle

# Custom CA Certificates Configuration
# Install custom CA certificates to system trust store
# This allows the system to trust self-signed or internal CA certificates
#
# Usage with tags:
#   ansible-playbook site.yml --tags ca-certificates    # Install CA certificates
#
enable_ca_certificates: false                          # Enable/disable CA certificate installation
ca_certificates: []
  # Example configurations:
  # - name: "internal-ca"                              # Certificate name (used for filename)
  #   content: |                                        # Inline PEM certificate content
  #     -----BEGIN CERTIFICATE-----
  #     MIIDxTCCAq2gAwIBAgIQAqxcJmoLQJuPC3nyrkYldzANBgkqhkiG9w0BAQsFADBs
  #     ...
  #     -----END CERTIFICATE-----
  # - name: "harbor-ca"
  #   url: "https://harbor.example.com/ca.crt"         # Download from URL
  # - name: "registry-ca"
  #   path: "/root/certs/my-ca.crt"                    # Copy from local path

# Domain-based Communication Configuration
# Enable domain-based communication for etcd and API server
# This allows cluster components to communicate using DNS names instead of IP addresses
# Useful for environments where IP addresses may change (cloud, VM migration, etc.)
#
# Usage with tags:
#   ansible-playbook site.yml --tags k8s-init    # Apply during cluster initialization
#
enable_domain_communication: true               # Enable/disable domain-based communication
domain_suffix: "k8s.local"                       # Domain suffix for cluster nodes (e.g., master1.k8s.local)
api_domain: "k8s-api.internal"                   # Domain for API server (used with load balancer/VIP)

# etcd Domain Configuration
# When enabled, etcd will advertise and communicate using domain names
# Format: {inventory_hostname}.{domain_suffix} (e.g., master1.k8s.local)
etcd_domain_config:
  # Listen on all interfaces for flexibility
  listen_peer_urls: "https://0.0.0.0:2380"
  listen_client_urls: "https://0.0.0.0:2379"
  # Cluster token for etcd
  initial_cluster_token: "etcd-cluster-1"
  initial_cluster_state: "new"

# Additional Certificate SANs for Domain Communication
# These domains will be added to etcd and API server certificates
# Automatically includes: {inventory_hostname}.{domain_suffix} for each master
# Add any additional domains here
extra_cert_sans:
  - "localhost"
  - "kubernetes"
  - "kubernetes.default"
  - "kubernetes.default.svc"
  - "kubernetes.default.svc.cluster.local"
  # Add custom domains as needed
  # - "k8s.example.com"

# ansible-playbook -i inventory.ini site.yml --tags coredns-hosts -e
